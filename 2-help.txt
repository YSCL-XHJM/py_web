clean_html_tags(text) 函数：
使用正则表达式匹配并移除输入文本中的所有HTML标签。
<.*?>: 这是正则表达式的模式，这里的点.表示匹配除了换行符之外的任何单个字符，
星号*表示前面的元素可以出现零次或多次，问号?使得这个量词变为非贪婪模式，即尽可能少地匹配字符直到遇到最近的右尖括号>为止。

remove_punctuation(text) 函数：
定义了一串包含多种标点符号的字符串。
使用正则表达式查找并替换掉文本中的所有标点符号，确保只留下纯文本内容。
r'[{}]+'.format(punctuation): 这部分构建了一个正则表达式模式。
r'...': 表示一个原始字符串，避免了转义字符的问题。
[...]: 定义了一个字符类，匹配方括号内的任意一个字符。
{}: 在这个上下文中是占位符，表示要被.format()方法替换的内容。
.format(punctuation): 这个方法会将punctuation字符串插入到正则表达式的字符类中，即把所有的标点符号放入方括号内。
+: 表示前面的元素（这里是方括号内的任意字符）可以连续出现一次或多次。这意味着任何连续的标点符号都会被一次性移除。

word_segmentation(text) 函数：
使用jieba.lcut()方法对文本进行分词。
过滤掉长度为1的词语，以减少噪音词汇（如“的”，“是”等常见但意义不大的词），只保留有意义的词语。
count_word_frequency(words) 函数：
使用Counter来统计每个词在列表中出现的次数，从而获得词频信息。
process_files(file_list, output_file='words.txt') 函数：
该函数接收一个文件名列表作为参数，并可选地接受一个输出文件名。
对每个文件执行以下操作：
尝试打开并读取文件的内容。
清理HTML标签。
移除标点符号。
分词并过滤。
将处理后的词添加到总词汇列表中。
如果有有效的词汇，则统计词频，并打印出前20个高频词汇。
将所有词及它们的频率写入指定的输出文件。
如果发生错误（例如文件不存在或编码问题），则捕获异常并打印错误信息。
if __name__ == "__main__":：
当这个脚本被直接运行时，这行代码之下的代码块将会被执行。
它定义了一个文件列表files，包含了要处理的文件名（'1.txt', '2.txt', '3.txt'）。
调用process_files(files)来开始处理这些文件。